{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9160a07b",
   "metadata": {},
   "source": [
    "To start,\n",
    "we'll just accumulate info in flat `pandas` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0473d438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>sha256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, source, sha256]\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "document_df = pd.DataFrame(columns=[\"text\", \"source\", \"sha256\"])\n",
    "\n",
    "document_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7d1058d",
   "metadata": {},
   "source": [
    "The Q&A will be more useful the more precisely we slice and link the documents,\n",
    "so we want to split a semantic \"document\", like a lecture or a video,\n",
    "up into sub-documents first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33186570",
   "metadata": {},
   "source": [
    "**Note**: we leave it up to the `langchain.TextSplitter` to split sub-documents into chunks smaller than a source at time of upsert into the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e512d62",
   "metadata": {},
   "source": [
    "## Markdown Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2f2e7",
   "metadata": {},
   "source": [
    "Most pages on the FSDL website\n",
    "are originally written in Markdown,\n",
    "which makes it easy to pull out relevant sub-documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736793f8",
   "metadata": {},
   "source": [
    "### Lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33fe14",
   "metadata": {},
   "source": [
    "We first define a `DataFrame` with basic metadata about where the lectures can be found -- on the website and as raw Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64cab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_md_url_base = \"https://raw.githubusercontent.com/ramnathv/corise-r-for-ds/main/notes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52a1a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url-slug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>week-00/01-welcome-to-r-for-ds/01-welcome-to-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>week-00/02-how-to-prepare-for-this-course/02-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>week-00/03-logistic-faqs/03-logistic-faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>week-01/01-doing-data-science/01-doing-data-sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>week-01/02-data-science-in-action/02-data-scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>week-01/03-importing-data/03-importing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>week-01/04-visualizing-data/04-visualizing-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>week-01/05-transforming-data/05-transforming-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>week-01/06-manipulating-data/06-manipulating-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>week-02/01-aggregating-data/01-aggregating-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>week-02/02-reshaping-data/02-reshaping-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>week-02/03-combining-data/03-combining-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>week-02/04-grammar-of-graphics/04-grammar-of-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>week-02/05-data-science-in-action-again/05-dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             url-slug\n",
       "1   week-00/01-welcome-to-r-for-ds/01-welcome-to-r...\n",
       "2   week-00/02-how-to-prepare-for-this-course/02-h...\n",
       "3        week-00/03-logistic-faqs/03-logistic-faqs.md\n",
       "4   week-01/01-doing-data-science/01-doing-data-sc...\n",
       "5   week-01/02-data-science-in-action/02-data-scie...\n",
       "6      week-01/03-importing-data/03-importing-data.md\n",
       "7   week-01/04-visualizing-data/04-visualizing-dat...\n",
       "8   week-01/05-transforming-data/05-transforming-d...\n",
       "9   week-01/06-manipulating-data/06-manipulating-d...\n",
       "10  week-02/01-aggregating-data/01-aggregating-dat...\n",
       "11     week-02/02-reshaping-data/02-reshaping-data.md\n",
       "12     week-02/03-combining-data/03-combining-data.md\n",
       "13  week-02/04-grammar-of-graphics/04-grammar-of-g...\n",
       "14  week-02/05-data-science-in-action-again/05-dat..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_slugs = {\n",
    "    1: \"week-00/01-welcome-to-r-for-ds/01-welcome-to-r-for-ds.md\",\n",
    "    2: \"week-00/02-how-to-prepare-for-this-course/02-how-to-prepare-for-this-course.md\",\n",
    "    3: \"week-00/03-logistic-faqs/03-logistic-faqs.md\",\n",
    "    4: \"week-01/01-doing-data-science/01-doing-data-science.md\",\n",
    "    5: \"week-01/02-data-science-in-action/02-data-science-in-action.md\",\n",
    "    6: \"week-01/03-importing-data/03-importing-data.md\",\n",
    "    7: \"week-01/04-visualizing-data/04-visualizing-data.md\",\n",
    "    8: \"week-01/05-transforming-data/05-transforming-data.md\",\n",
    "    9: \"week-01/06-manipulating-data/06-manipulating-data.md\",\n",
    "    10: \"week-02/01-aggregating-data/01-aggregating-data.md\",\n",
    "    11: \"week-02/02-reshaping-data/02-reshaping-data.md\",\n",
    "    12: \"week-02/03-combining-data/03-combining-data.md\",\n",
    "    13: \"week-02/04-grammar-of-graphics/04-grammar-of-graphics.md\",\n",
    "    14: \"week-02/05-data-science-in-action-again/05-data-science-in-action-again.md\"\n",
    "}\n",
    "\n",
    "notes_df = pd.DataFrame.from_dict(notes_slugs, orient=\"index\", columns=[\"url-slug\"])\n",
    "notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ada092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df[\"raw-md-url\"] = notes_df[\"url-slug\"].apply(lambda s: f\"{notes_md_url_base}/{s}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f981b5",
   "metadata": {},
   "source": [
    "We then bring in the markdown files from GitHub,\n",
    "parse them to split out headings as our \"sources\",\n",
    "and use `slugify` to create URLs for those heading sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19becb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "\n",
    "\n",
    "def get_text_from(url):\n",
    "    with open(url) as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "notes_df[\"raw-text\"] = notes_df[\"raw-md-url\"].apply(lambda url: get_text_from(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a2a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistune\n",
    "from slugify import slugify\n",
    "\n",
    "\n",
    "def get_target_headings_and_slugs(text):\n",
    "    markdown_parser = mistune.create_markdown(renderer=\"ast\")\n",
    "    parsed_text = markdown_parser(text)\n",
    "    \n",
    "    heading_objects = [obj for obj in parsed_text if obj[\"type\"] == \"heading\"]\n",
    "    h2_objects = [obj for obj in heading_objects if obj[\"level\"] == 2]\n",
    "    \n",
    "    targets = [obj for obj in h2_objects if not(obj[\"children\"][0][\"text\"].startswith(\"description: \"))]\n",
    "    target_headings = [tgt[\"children\"][0][\"text\"] for tgt in targets]\n",
    "    \n",
    "    heading_slugs = [slugify(target_heading) for target_heading in target_headings]\n",
    "    \n",
    "    return target_headings, heading_slugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7987faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_notes(row):\n",
    "    text = row[\"raw-text\"]\n",
    "    \n",
    "    headings, slugs = get_target_headings_and_slugs(text)\n",
    "    \n",
    "    texts = split_by_headings(text, headings)\n",
    "    slugs = [\"\"] + slugs\n",
    "    \n",
    "    text_rows = []\n",
    "    for text, slug in zip(texts, slugs):\n",
    "        text_rows.append({\n",
    "            \"url-slug\": row[\"url-slug\"] + \"#\" + slug,\n",
    "            \"raw-md-url\": row[\"raw-md-url\"],\n",
    "            \"text\": text,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame.from_records(text_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0633af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_headings(text, headings):\n",
    "    texts = []\n",
    "    for heading in reversed(headings):\n",
    "        text, section = text.split(\"# \" + heading)\n",
    "        texts.append(f\"## {heading}{section}\")\n",
    "    texts.append(text)\n",
    "    texts = list(reversed(texts))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "486b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_dfs = []\n",
    "for idx, row in notes_df.iterrows():\n",
    "    single_note_df = split_notes(row)\n",
    "    single_note_df[\"notes-idx\"] = idx\n",
    "    note_dfs.append(single_note_df)\n",
    "    \n",
    "split_notes_df = pd.concat(note_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "447af422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url-slug</th>\n",
       "      <th>raw-md-url</th>\n",
       "      <th>text</th>\n",
       "      <th>notes-idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>week-00/01-welcome-to-r-for-ds/01-welcome-to-r...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n### 👋 Hi!!\\n\\nHello, I’m Ramnath, and I’m ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>week-00/02-how-to-prepare-for-this-course/02-h...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n### How to Prepare for This Course?\\n\\nWe cr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>week-00/03-logistic-faqs/03-logistic-faqs.md#</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n### Are sessions recorded?\\n\\nWe encourage y...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>week-01/01-doing-data-science/01-doing-data-sc...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>week-01/01-doing-data-science/01-doing-data-sc...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Doing Data Science\\n\\n### What is Data Scie...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>week-01/02-data-science-in-action/02-data-scie...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>week-01/02-data-science-in-action/02-data-scie...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Data Science in Action\\n\\nThe best way to g...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>week-01/03-importing-data/03-importing-data.md#</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>week-01/03-importing-data/03-importing-data.md...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Importing Data\\n\\nImporting data refers to ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>week-01/04-visualizing-data/04-visualizing-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>week-01/04-visualizing-data/04-visualizing-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Visualizing Data\\n\\nData visualization is a...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>week-01/05-transforming-data/05-transforming-d...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>week-01/05-transforming-data/05-transforming-d...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Transforming Data\\n\\nVisualizing data is a ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>week-01/06-manipulating-data/06-manipulating-d...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>week-01/06-manipulating-data/06-manipulating-d...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Manipulating Data\\n\\nRecall how data manipu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>week-02/01-aggregating-data/01-aggregating-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>week-02/01-aggregating-data/01-aggregating-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Aggregating Data\\n\\n**Aggregating** data in...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>week-02/02-reshaping-data/02-reshaping-data.md#</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>week-02/02-reshaping-data/02-reshaping-data.md...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Reshaping Data\\n\\n**Reshaping** data is a f...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>week-02/03-combining-data/03-combining-data.md#</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>week-02/03-combining-data/03-combining-data.md...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Combining Data\\n\\n**Combining** data spread...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>week-02/04-grammar-of-graphics/04-grammar-of-g...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>week-02/04-grammar-of-graphics/04-grammar-of-g...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Grammar of Graphics\\n\\nThe **Grammar of Gra...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>week-02/05-data-science-in-action-again/05-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>\\n#</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>week-02/05-data-science-in-action-again/05-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Unisex Names\\n\\n&lt;img src=\"https://fivethirt...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>week-02/05-data-science-in-action-again/05-dat...</td>\n",
       "      <td>https://raw.githubusercontent.com/ramnathv/cor...</td>\n",
       "      <td>## Unisex Names Trends\\n\\nWhat if we wanted to...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             url-slug   \n",
       "0   week-00/01-welcome-to-r-for-ds/01-welcome-to-r...  \\\n",
       "1   week-00/02-how-to-prepare-for-this-course/02-h...   \n",
       "2       week-00/03-logistic-faqs/03-logistic-faqs.md#   \n",
       "3   week-01/01-doing-data-science/01-doing-data-sc...   \n",
       "4   week-01/01-doing-data-science/01-doing-data-sc...   \n",
       "5   week-01/02-data-science-in-action/02-data-scie...   \n",
       "6   week-01/02-data-science-in-action/02-data-scie...   \n",
       "7     week-01/03-importing-data/03-importing-data.md#   \n",
       "8   week-01/03-importing-data/03-importing-data.md...   \n",
       "9   week-01/04-visualizing-data/04-visualizing-dat...   \n",
       "10  week-01/04-visualizing-data/04-visualizing-dat...   \n",
       "11  week-01/05-transforming-data/05-transforming-d...   \n",
       "12  week-01/05-transforming-data/05-transforming-d...   \n",
       "13  week-01/06-manipulating-data/06-manipulating-d...   \n",
       "14  week-01/06-manipulating-data/06-manipulating-d...   \n",
       "15  week-02/01-aggregating-data/01-aggregating-dat...   \n",
       "16  week-02/01-aggregating-data/01-aggregating-dat...   \n",
       "17    week-02/02-reshaping-data/02-reshaping-data.md#   \n",
       "18  week-02/02-reshaping-data/02-reshaping-data.md...   \n",
       "19    week-02/03-combining-data/03-combining-data.md#   \n",
       "20  week-02/03-combining-data/03-combining-data.md...   \n",
       "21  week-02/04-grammar-of-graphics/04-grammar-of-g...   \n",
       "22  week-02/04-grammar-of-graphics/04-grammar-of-g...   \n",
       "23  week-02/05-data-science-in-action-again/05-dat...   \n",
       "24  week-02/05-data-science-in-action-again/05-dat...   \n",
       "25  week-02/05-data-science-in-action-again/05-dat...   \n",
       "\n",
       "                                           raw-md-url   \n",
       "0   https://raw.githubusercontent.com/ramnathv/cor...  \\\n",
       "1   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "2   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "3   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "4   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "5   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "6   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "7   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "8   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "9   https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "10  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "11  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "12  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "13  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "14  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "15  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "16  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "17  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "18  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "19  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "20  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "21  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "22  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "23  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "24  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "25  https://raw.githubusercontent.com/ramnathv/cor...   \n",
       "\n",
       "                                                 text  notes-idx  \n",
       "0   \\n### 👋 Hi!!\\n\\nHello, I’m Ramnath, and I’m ex...          1  \n",
       "1   \\n### How to Prepare for This Course?\\n\\nWe cr...          2  \n",
       "2   \\n### Are sessions recorded?\\n\\nWe encourage y...          3  \n",
       "3                                                 \\n#          4  \n",
       "4   ## Doing Data Science\\n\\n### What is Data Scie...          4  \n",
       "5                                                 \\n#          5  \n",
       "6   ## Data Science in Action\\n\\nThe best way to g...          5  \n",
       "7                                                 \\n#          6  \n",
       "8   ## Importing Data\\n\\nImporting data refers to ...          6  \n",
       "9                                                 \\n#          7  \n",
       "10  ## Visualizing Data\\n\\nData visualization is a...          7  \n",
       "11                                                \\n#          8  \n",
       "12  ## Transforming Data\\n\\nVisualizing data is a ...          8  \n",
       "13                                                \\n#          9  \n",
       "14  ## Manipulating Data\\n\\nRecall how data manipu...          9  \n",
       "15                                                \\n#         10  \n",
       "16  ## Aggregating Data\\n\\n**Aggregating** data in...         10  \n",
       "17                                                \\n#         11  \n",
       "18  ## Reshaping Data\\n\\n**Reshaping** data is a f...         11  \n",
       "19                                                \\n#         12  \n",
       "20  ## Combining Data\\n\\n**Combining** data spread...         12  \n",
       "21                                                \\n#         13  \n",
       "22  ## Grammar of Graphics\\n\\nThe **Grammar of Gra...         13  \n",
       "23                                                \\n#         14  \n",
       "24  ## Unisex Names\\n\\n<img src=\"https://fivethirt...         14  \n",
       "25  ## Unisex Names Trends\\n\\nWhat if we wanted to...         14  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c3b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "doc_ids = []\n",
    "for _, row in split_notes_df.iterrows():\n",
    "    m = hashlib.sha256()\n",
    "    m.update(row[\"text\"].encode(\"utf-8\"))\n",
    "    doc_ids.append(m.hexdigest())\n",
    "    \n",
    "split_notes_df.index = doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ee99e",
   "metadata": {},
   "source": [
    "## Persist to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc2f61",
   "metadata": {},
   "source": [
    "As a first step to persisting our corpus,\n",
    "let's save it to disk and reload it.\n",
    "\n",
    "The data involved is relatively simple --\n",
    "basically all strings --\n",
    "so we don't need to `pickle` the `DataFrame`,\n",
    "which comes with its own woes.\n",
    "\n",
    "Instead, we just format it as `JSON` --\n",
    "the web's favorite serialization format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5513f7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame index must be unique for orient='index'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m document_json \u001b[39m=\u001b[39m split_notes_df\u001b[39m.\u001b[39;49mto_json(orient\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:2532\u001b[0m, in \u001b[0;36mNDFrame.to_json\u001b[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[1;32m   2529\u001b[0m config\u001b[39m.\u001b[39mis_nonnegative_int(indent)\n\u001b[1;32m   2530\u001b[0m indent \u001b[39m=\u001b[39m indent \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 2532\u001b[0m \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mto_json(\n\u001b[1;32m   2533\u001b[0m     path_or_buf\u001b[39m=\u001b[39;49mpath_or_buf,\n\u001b[1;32m   2534\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2535\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m   2536\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   2537\u001b[0m     double_precision\u001b[39m=\u001b[39;49mdouble_precision,\n\u001b[1;32m   2538\u001b[0m     force_ascii\u001b[39m=\u001b[39;49mforce_ascii,\n\u001b[1;32m   2539\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m   2540\u001b[0m     default_handler\u001b[39m=\u001b[39;49mdefault_handler,\n\u001b[1;32m   2541\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[1;32m   2542\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2543\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2544\u001b[0m     indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m   2545\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2546\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   2547\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:181\u001b[0m, in \u001b[0;36mto_json\u001b[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobj\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should be a Series or a DataFrame\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m s \u001b[39m=\u001b[39m writer(\n\u001b[1;32m    182\u001b[0m     obj,\n\u001b[1;32m    183\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m    184\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m    185\u001b[0m     double_precision\u001b[39m=\u001b[39;49mdouble_precision,\n\u001b[1;32m    186\u001b[0m     ensure_ascii\u001b[39m=\u001b[39;49mforce_ascii,\n\u001b[1;32m    187\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m    188\u001b[0m     default_handler\u001b[39m=\u001b[39;49mdefault_handler,\n\u001b[1;32m    189\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    190\u001b[0m     indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    191\u001b[0m )\u001b[39m.\u001b[39mwrite()\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m lines:\n\u001b[1;32m    194\u001b[0m     s \u001b[39m=\u001b[39m convert_to_line_delimits(s)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:237\u001b[0m, in \u001b[0;36mWriter.__init__\u001b[0;34m(self, obj, orient, date_format, double_precision, ensure_ascii, date_unit, index, default_handler, indent)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m=\u001b[39m indent\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_copy \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_axes()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:293\u001b[0m, in \u001b[0;36mFrameWriter._format_axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mTry to format axes if they are datelike.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    294\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame index must be unique for orient=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient \u001b[39min\u001b[39;00m (\n\u001b[1;32m    297\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    298\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    300\u001b[0m ):\n\u001b[1;32m    301\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame columns must be unique for orient=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame index must be unique for orient='index'."
     ]
    }
   ],
   "source": [
    "documents_json = split_notes_df.to_json(orient=\"index\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed747fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents.json\", \"w\") as f:\n",
    "    f.write(documents_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2eb6a",
   "metadata": {},
   "source": [
    "Before moving on,\n",
    "let's check that we can in fact reload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66e8bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"documents.json\") as f:\n",
    "    s = f.read()\n",
    "    \n",
    "key, document = list(json.loads(s).items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760de973",
   "metadata": {},
   "source": [
    "## Put into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4939c2",
   "metadata": {},
   "source": [
    "But a local filesystem isn't a good method for persistence.\n",
    "\n",
    "We want these documents to be available via an API,\n",
    "with the ability to scale reads and writes if needed.\n",
    "\n",
    "So let's put them in a database.\n",
    "\n",
    "We choose MongoDB simply for convenience --\n",
    "we don't want to define a schema just yet,\n",
    "since these tools are evolving rapidly,\n",
    "and there are nice free hosting options.\n",
    "\n",
    "> MongoDB is, in NoSQL terms, a \"document database\",\n",
    "but the term document means something different\n",
    "than it does in \"Document Q&A\".\n",
    "In Mongoland, a \"document\" is just a blob of JSON.\n",
    "We format our Q&A documents as JSON\n",
    "and store them in Mongo,\n",
    "so the distinction is not obvious here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6dd65",
   "metadata": {},
   "source": [
    "If you're running this yourself,\n",
    "you'll need to create a hosted MongoDB instance\n",
    "and add a database called `fsdl`\n",
    "with a collection called `ask-fsdl`.\n",
    "\n",
    "You can find instructions\n",
    "[here](https://www.mongodb.com/basics/mongodb-atlas-tutorial).\n",
    "\n",
    "You'll need the URL and password info\n",
    "from that setup process to connect.\n",
    "\n",
    "Add them to the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b989c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['ac-mmh8jah-shard-00-00.6fpuqjc.mongodb.net:27017', 'ac-mmh8jah-shard-00-02.6fpuqjc.mongodb.net:27017', 'ac-mmh8jah-shard-00-01.6fpuqjc.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', authsource='admin', replicaset='atlas-q5qzwz-shard-0', tls=True), 'Cluster0'), 'Cluster0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "from pymongo import InsertOne\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongodb_url = os.environ[\"MONGODB_URI\"]\n",
    "mongodb_password = os.environ[\"MONGODB_PASSWORD\"]\n",
    "\n",
    "CONNECTION_STRING = os.environ[\"MONGODB_URI\"]\n",
    "\n",
    "# connect to the database server\n",
    "client = pymongo.MongoClient(CONNECTION_STRING)\n",
    "# connect to the database\n",
    "db = client.get_database(\"Cluster0\")\n",
    "# get a representation of the collection\n",
    "collection = db.get_collection(\"Cluster0\")\n",
    "\n",
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e779e",
   "metadata": {},
   "source": [
    "Now that we're connected,\n",
    "we're ready to upsert.\n",
    "\n",
    "We loop over the documents -- loaded from disk --\n",
    "and format them into a Python dictionary\n",
    "that fits our `Document` pseudoschema.\n",
    "\n",
    "With `pymongo`,\n",
    "we can just insert that dictionary directly,\n",
    "using `InsertOne`,\n",
    "and use `bulk_write` to get batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 250\n",
    "requesting = []\n",
    "\n",
    "with open(\"documents.json\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "\n",
    "for (sha_hash, content) in documents.items():\n",
    "    metadata = {key: value for key, value in content.items() if key != \"text\"}\n",
    "    metadata[\"sha256\"] = sha_hash\n",
    "    document = {\"text\": content[\"text\"], \"metadata\": metadata}\n",
    "    requesting.append(InsertOne(document))\n",
    "    \n",
    "    if len(requesting) >= CHUNK_SIZE:\n",
    "        collection.bulk_write(requesting)\n",
    "        requesting = []\n",
    "        \n",
    "if requesting:\n",
    "    collection.bulk_write(requesting)\n",
    "    requesting = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
